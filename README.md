ğŸ§  Code Evaluation Orchestra
<br>
AI Agents Assemble â€” Hackathon Submission
<br> 
ğŸ”— Live Demo: https://mecode-evaluation-orchestra.vercel.app/
<br><br>
ğŸš€ Overview
<br> 
Code Evaluation Orchestra is an AI-agent-inspired coding practice and interview evaluation platform.
Users choose a problem, write a solution, and receive structured, interview-style feedback instead of only pass/fail results.
<br> 
The project simulates how multiple AI agents collaborate to evaluate code â€” similar to how real interviewers assess solutions.
<br><br> 
â“ Problem Statement
<br>
Learners often struggle to understand:
<br>
-What logic was missing
<br>
-Which edge cases failed
<br>
-Whether their solution is interview-ready
<br>
Code Evaluation Orchestra addresses this gap by emphasizing clarity, feedback, and explanation.
<br><br>
### ğŸ¤– AI Agent Architecture

The system is designed as a collaborative multi-agent workflow:

| Agent          | Responsibility                                  |
|---------------|--------------------------------------------------|
| Test Generator | Provides problem-specific test cases            |
| Code Analyzer  | Inspects solution logic patterns                |
| Evaluator      | Assigns a score based on correctness            |
| Summarizer     | Generates interview-style feedback              |

Together, these agents produce a single, clear, and understandable evaluation for the user.
<br><br>

âœ¨ Key Features

- Multiple Coding Problems  
  A curated set of coding challenges across different difficulty levels.

- Clear Problem Statements  
  Each problem includes a concise and well-defined description.

- Interactive Code Editor  
  Users can write and edit solutions directly in the browser.

- Starter Code Templates  
  Predefined function signatures to guide candidates.

- Problem-Specific Test Cases  
  Each problem includes visible test cases.

- Scoring System  
  Solutions are scored based on correctness and quality.

- Technical Analysis  
  Explains why a solution passes or fails.

- Interview-Style Feedback  
  Human-readable feedback similar to real interviews.
<br><br>
ğŸ§ªExample User Flow
  
Select a coding problem
<br>
Read the problem statement
<br>
Write a solution
<br.
Click Run Evaluation
<br>
View score, test results, analysis, and summary
<br><br>
ğŸ§© Sponsor Tool Usage

âš¡ Stormbreaker Deployment (Vercel)
Used for fast and reliable deployment.

ğŸ§  Technical analysis of the solution

ğŸ Results & Impact

Improves learning by explaining why a solution works or fails

Mimics real interview evaluation

Encourages better problem-solving habits
<br><br>
ğŸ‘©â€ğŸ’» Author

Dimple Goyal
GitHub: https://github.com/me-Dimple72

ğŸ—£ Interview-style summary feedback

