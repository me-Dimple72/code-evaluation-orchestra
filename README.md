ğŸ§  Code Evaluation Orchestra
<br>
AI Agents Assemble â€” Hackathon Submission
<br> 
ğŸ”— Live Demo: https://mecode-evaluation-orchestra.vercel.app/
<br><br>
ğŸš€ Overview
<br> 
Code Evaluation Orchestra is an AI-agent-inspired coding practice and interview evaluation platform.
Users choose a problem, write a solution, and receive structured, interview-style feedback instead of only pass/fail results.
<br> 
The project simulates how multiple AI agents collaborate to evaluate code â€” similar to how real interviewers assess solutions.
<br><br> 
â“ Problem Statement
<br>
Learners often struggle to understand:
<br>
-What logic was missing
<br>
-Which edge cases failed
<br>
-Whether their solution is interview-ready
<br>
Code Evaluation Orchestra addresses this gap by emphasizing clarity, feedback, and explanation.
<br><br>
ğŸ¤– AI Agent Architecture (Conceptual)
<br>
The system is designed as a collaborative agent workflow:

Agent              |    	Responsibility
<br>               |
Test Generator	   |      Provides problem-specific test cases
<br>               |
Code Analyzer	     |      Inspects solution logic patterns
<br>
Evaluator	         |      Assigns a score based on correctness
<br>
Summarizer	       |      Generates interview-style feedback
<br>
Together, these agents produce a single, understandable evaluation for the user.
<br><br>
âœ¨ Key Features

ğŸ“Œ Multiple coding problems (Easy â†’ Medium)

ğŸ“ Clear problem statements

ğŸ’» Interactive code editor with starter code

ğŸ§ª Visible test cases for each problem

ğŸ“Š Score and pass/fail breakdown

ğŸ§  Technical analysis of the solution

ğŸ—£ Interview-style summary feedback

